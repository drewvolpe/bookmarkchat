âœ… MVP Build Plan (Granular Steps)
ğŸ“¦ 1. Project Setup
1.1 Initialize Monorepo
Create root project folder: bookmark-chat-app

Create subfolders: /apps/web, /apps/api, /packages/fetcher, /packages/embedder, /data/cache, /scripts

Initialize Git + root README.md

âœ… Done when: Folders exist and git status is clean.

ğŸ§ª 2. Pinboard Bookmark Sync
2.1 Create .env file with Pinboard token
Store token as PINBOARD_TOKEN=your_token_here

âœ… Done when: .env exists and is ignored via .gitignore

2.2 Write pinboard_fetcher.py to fetch bookmarks
Load token from .env

Call Pinboard /posts/all

Save bookmarks to data/cache/bookmarks.json

âœ… Done when: JSON file is written with valid bookmarks

2.3 Add caching check to pinboard_fetcher.py
Skip fetching if cache file exists and --force not passed

âœ… Done when: Re-running script without --force skips network call

ğŸŒ 3. Webpage Content Fetching
3.1 Write page_fetcher.py to fetch + extract readable text
Input: URL

Output: Clean page text (use newspaper3k or readability-lxml)

Save to data/cache/pages/<url_hash>.txt

âœ… Done when: Text file written for at least 1 URL

3.2 Write loop to fetch all bookmarks' pages
Read from bookmarks.json

Download + save content for each (respect cache)

âœ… Done when: Local folder has .txt files for multiple URLs

ğŸ§  4. Embedding + Vector Search
4.1 Add chunker.py to split page text into chunks
Input: text string

Output: list of chunks (~500 tokens)

âœ… Done when: Unit test shows 2+ chunks per ~1000-word input

4.2 Add embedder.py to call OpenAI embeddings API
Input: list of text chunks

Output: list of vectors

âœ… Done when: Embeddings returned and printed (use mock mode if no key)

4.3 Add vectorstore.py with basic FAISS support
Store: (vector, metadata)

Query: input vector â†’ top-K results

âœ… Done when: Search returns relevant chunks from stored data

4.4 Write sync_bookmarks.py
Combine:

Fetch bookmarks

Fetch pages

Chunk + embed

Store in FAISS

âœ… Done when: Running script populates FAISS index

ğŸ§  5. Backend API
5.1 Set up apps/api with FastAPI
Create main.py and simple root endpoint

âœ… Done when: curl localhost:8000/ returns {"message":"ok"}

5.2 Add /chat endpoint
Accepts: POST { query: string }

Embeds query â†’ vector search â†’ return top K results

âœ… Done when: Response includes relevant chunk metadata (not LLM yet)

5.3 Add LLM call to /chat
Use OpenAI or mock

Return synthesized response based on top chunks

âœ… Done when: Query returns paragraph answer with sources

ğŸ’» 6. Frontend (Next.js)
6.1 Initialize Next.js app in apps/web
Create page: pages/index.tsx

Render input box + message list

âœ… Done when: Typing input updates component state

6.2 Connect frontend to /chat endpoint
On submit, send query â†’ render response

âœ… Done when: Typing question shows chat response

6.3 Show bookmark metadata with response
Title + URL alongside LLM response

âœ… Done when: Chat shows source links

ğŸ§¹ 7. Final MVP Polish
7.1 Add loading states + error handling to UI
âœ… Done when: UX doesnâ€™t break on slow/missing responses

7.2 Add simple search/filter UI for bookmarks
âœ… Done when: Can search list of bookmarks by keyword/tag

7.3 Write README with setup/run instructions
âœ… Done when: Clone â†’ run works end to end